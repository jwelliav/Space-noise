{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks to generate dark data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a Generative adversarial network with the goal of generating dark data of the WFC3 camera\n",
    "in the Hubble space telescope. We gather the data from the MAST archive. We separate out the files obtained into flt and flc files \n",
    "and in this notebook proceed as follows. \n",
    "\n",
    "1. We create the dataset we are interested in. By this we mean, we extract numpy arrays from the flt files which are in FITS format and take every such numpy image and cut it into chunks of size 64*64.\n",
    "\n",
    "2. We try and understand the distribution of data in the images, and calculate the mean and variance of the distributions. These values will be of use later when we normalize the data. \n",
    "\n",
    "3. We create dataloader which we use to feed into the GAN. Note that this requires a custom loader since we work with numpy arrays.  \n",
    "\n",
    "4. We create the discrimator and generator. The architecture for both networks is along the lines of pytorch examples. \n",
    "\n",
    "5. We then train the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataset for GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We creat the GAN dataset consisting of numpy arrays extracted from a collection of FITS files which we have downloaded from the MAST archive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/johnwelliaveetil/mastDownload/flt_images'\n",
    "path0 = '/Users/johnwelliaveetil/mastDownload'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test a snippet of code that extracts the image data (numpy array) from one of the FITS files and cuts it into pieces, each of which are of size 1024*1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/johnwelliaveetil/mastDownload/flt_images/ib918lsuq_flt.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU     262   ()      \n",
      "  1  SCI           1 ImageHDU        90   (4096, 2051)   float32   \n",
      "  2  ERR           1 ImageHDU        43   (4096, 2051)   float32   \n",
      "  3  DQ            1 ImageHDU        35   (4096, 2051)   int16   \n",
      "  4  SCI           2 ImageHDU        90   (4096, 2051)   float32   \n",
      "  5  ERR           2 ImageHDU        43   (4096, 2051)   float32   \n",
      "  6  DQ            2 ImageHDU        35   (4096, 2051)   int16   \n"
     ]
    }
   ],
   "source": [
    "path_sample = '/Users/johnwelliaveetil/mastDownload/flt_images/ib918lsuq_flt.fits'\n",
    "hdul = fits.open(path_sample)\n",
    "hdul.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2051, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = hdul[1].data\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that takes as input an image and cuts into pieces each of which are of size 1024*1024. The string s here is usually the name of the image file. The output is a dictionnary where the keys are the 'locations' of the sub images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sub images of a given image. We would like all our inputs to be 64*64\n",
    "def get_sub_images(X,s):\n",
    "    A = {}\n",
    "    t = s[0:-5]\n",
    "    a,b = X.shape\n",
    "    if min(a,b) >= 64:\n",
    "        m = math.floor(a/64)\n",
    "        n = math.floor(b/64)\n",
    "        for i in range(1,m+1):\n",
    "            for j in range(1,n+1):\n",
    "                temp = X[64*(i-1):64*i,64*(j-1):64*j]\n",
    "                A[t + '_' + str(i)+str(j)] = temp\n",
    "    return(A)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the code against the sample image drawn previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = get_sub_images(image,'t')\n",
    "#A.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.170859  ,  0.07856879, -2.1860962 , ..., -4.9314356 ,\n",
       "        -3.8910155 , -4.9827304 ],\n",
       "       [-2.1201289 ,  1.8412665 ,  0.0212412 , ..., -2.0411713 ,\n",
       "        -1.1985307 ,  1.1639295 ],\n",
       "       [-1.0004331 , -1.075549  , -4.049245  , ...,  2.1213102 ,\n",
       "         1.0418154 ,  2.2270863 ],\n",
       "       ...,\n",
       "       [ 0.7554387 ,  0.994149  , -1.1099977 , ..., -1.1025146 ,\n",
       "        -1.1983027 , -2.0856662 ],\n",
       "       [ 0.91668344,  3.8717203 ,  1.961451  , ..., -2.075353  ,\n",
       "         0.0729558 , -0.05109308],\n",
       "       [ 0.8697649 ,  4.912557  , -1.9528145 , ..., -3.2480972 ,\n",
       "        -2.0225523 , -0.09782083]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A['_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('>f4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A['_11'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function that takes as input a dictionnary (as given by the function get_sub_images) and outputs a npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sub_image(C):\n",
    "    for j in C.keys():\n",
    "        name = '/Users/johnwelliaveetil/mastDownload/np_images_64' + '/' + j + '.npy'\n",
    "        np.save(name,C[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with the dictionnary A from above......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sub_image(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the save np images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_saved = (np.load('/Users/johnwelliaveetil/mastDownload/np_images_64/_11.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.170859  ,  0.07856879, -2.1860962 , ..., -4.9314356 ,\n",
       "        -3.8910155 , -4.9827304 ],\n",
       "       [-2.1201289 ,  1.8412665 ,  0.0212412 , ..., -2.0411713 ,\n",
       "        -1.1985307 ,  1.1639295 ],\n",
       "       [-1.0004331 , -1.075549  , -4.049245  , ...,  2.1213102 ,\n",
       "         1.0418154 ,  2.2270863 ],\n",
       "       ...,\n",
       "       [ 0.7554387 ,  0.994149  , -1.1099977 , ..., -1.1025146 ,\n",
       "        -1.1983027 , -2.0856662 ],\n",
       "       [ 0.91668344,  3.8717203 ,  1.961451  , ..., -2.075353  ,\n",
       "         0.0729558 , -0.05109308],\n",
       "       [ 0.8697649 ,  4.912557  , -1.9528145 , ..., -3.2480972 ,\n",
       "        -2.0225523 , -0.09782083]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_saved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(image_saved, 'RGB')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like that worked !\n",
    "\n",
    "We now write a function that runs over the entire set of FITS dataset and for each element of the dataset, it extracts the image data and cuts into pieces of the required size. \n",
    "\n",
    "Note : We do not consider files which are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: File may have been truncated: actual file length (166962726) is smaller than the expected size (168076800) [astropy.io.fits.file]\n",
      "WARNING: File may have been truncated: actual file length (164296728) is smaller than the expected size (168076800) [astropy.io.fits.file]\n",
      "WARNING: File may have been truncated: actual file length (6093936) is smaller than the expected size (33635520) [astropy.io.fits.file]\n"
     ]
    }
   ],
   "source": [
    "A = os.listdir(path)\n",
    "A.remove(A[60])\n",
    "A.remove(A[66])\n",
    "count = 0\n",
    "for a in A:\n",
    "#   print(count)\n",
    "    hdul = fits.open(path + '/' + a)\n",
    "    if len(hdul) == 7:\n",
    "        image1 = hdul[1].data\n",
    "        image2 = hdul[4].data\n",
    "        image1 = image1.byteswap().newbyteorder()\n",
    "        image2 = image2.byteswap().newbyteorder()\n",
    "        s1 = a[:-5] + '_1.fits'\n",
    "        s2 = a[:-5] + '_2.fits'\n",
    "        C = get_sub_images(image1,s1)\n",
    "        save_sub_image(C)\n",
    "        D = get_sub_images(image2,s2)\n",
    "        save_sub_image(D)\n",
    "    if len(hdul) == 4:\n",
    "        image1 = hdul[1].data\n",
    "        image1 = image1.byteswap().newbyteorder()\n",
    "        C = get_sub_images(image1,a)\n",
    "        save_sub_image(C)        \n",
    "    hdul.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the GAN Dataset !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelimnary analysis on the image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load random selected images from the folder and look at how the float values that make up the image vary\n",
    "within a given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = os.listdir('/Users/johnwelliaveetil/mastDownload/np_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a random image from the list B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35599  ---> ibc131enq_flt_1_3235.npy\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(len(B))\n",
    "image_sample = np.load('/Users/johnwelliaveetil/mastDownload/np_images_64'+'/' + B[0])\n",
    "print(i,' ---> ' + B[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max = 6665.462891\n",
      "Min = -5.628556\n",
      "Mean = 13.547943\n",
      "Standard Deviation = 191.073532\n"
     ]
    }
   ],
   "source": [
    "A1 = image_sample.flatten()\n",
    "print('Max = %f' %A1.max())\n",
    "print('Min = %f' %A1.min())\n",
    "print('Mean = %f' %A1.mean())\n",
    "print('Standard Deviation = %f' %A1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.percentile(image_sample,95)\n",
    "B1 = [x for x in A1 if x < a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEaJJREFUeJzt3X+MZWV9x/H3p4ho1BYpI1131w61qwZtWciIGPoDpSo/jIuJJZAUkNKsGjCYkCpgGm1SEtoqFNOWdBUUWxQJYtkoWhFpjX8ADrjyU+oWoexmYUcFxJBigG//mLM44uzeO3Pv3Tvz7PuVTObc5/y43zO793Ofee5zzqSqkCS169fGXYAkabQMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjntdrgyQvAL4F7NNtf3VVfSTJZ4A/Bh7rNn13VW1KEuBi4Fjgia79tl09x/7771+Tk5OLPglJ2hPdeuutP6qqiV7b9Qx64EngzVX1syR7A99O8tVu3V9W1dXP2f4YYE339Qbgku77Tk1OTjI9Pd1HKZKkHZI80M92PYduatbPuod7d1+7ukHOOuCz3X43AfsmWdFPMZKk4etrjD7JXkk2AduB66vq5m7V+UluT3JRkn26tpXAg3N239K1PfeY65NMJ5memZkZ4BQkSbvSV9BX1dNVtRZYBRyW5HXAucBrgNcD+wEfWsgTV9WGqpqqqqmJiZ5DTJKkRVrQrJuqehS4ETi6qrZ1wzNPAp8GDus22wqsnrPbqq5NkjQGPYM+yUSSfbvlFwJvAb6/Y9y9m2VzPHBnt8tG4JTMOhx4rKq2jaR6SVJP/cy6WQFcnmQvZt8YrqqqLyf5ZpIJIMAm4L3d9tcxO7VyM7PTK08bftmSpH71DPqquh04ZJ72N+9k+wLOGLw0SdIweGWsJDXOoJekxvUzRi9pASbP+cqzy/dfcNwYK5Fm2aOXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuO8qZma503GtKezRy9JjTPoJalxBr0kNc6gl6TGGfSS1LieQZ/kBUluSfK9JHcl+euu/cAkNyfZnOQLSZ7fte/TPd7crZ8c7SlIknalnx79k8Cbq+pgYC1wdJLDgb8FLqqq3wUeAU7vtj8deKRrv6jbTpI0Jj2Dvmb9rHu4d/dVwJuBq7v2y4Hju+V13WO69UclydAqliQtSF9j9En2SrIJ2A5cD/wP8GhVPdVtsgVY2S2vBB4E6NY/BvzmMIuWJPWvr6Cvqqerai2wCjgMeM2gT5xkfZLpJNMzMzODHk6StBMLmnVTVY8CNwJvBPZNsuMWCquArd3yVmA1QLf+N4Afz3OsDVU1VVVTExMTiyxfktRLP7NuJpLs2y2/EHgLcA+zgf+ubrNTgWu75Y3dY7r136yqGmbRkqT+9XNTsxXA5Un2YvaN4aqq+nKSu4Erk/wN8F3g0m77S4F/TbIZ+Alw4gjqliT1qWfQV9XtwCHztN/H7Hj9c9v/D/jToVQnSRqYV8ZKUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4fi6YkrRETZ7zlWeX77/guDFWoqXMHr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJalzPoE+yOsmNSe5OcleSs7r2jybZmmRT93XsnH3OTbI5yb1J3jbKE5Ak7Vo/96N/Cji7qm5L8hLg1iTXd+suqqqPzd04yUHAicBrgZcD30jyqqp6epiFS5L607NHX1Xbquq2bvlx4B5g5S52WQdcWVVPVtUPgc3AYcMoVpK0cAsao08yCRwC3Nw1nZnk9iSXJXlp17YSeHDOblvY9RuDJGmE+g76JC8Gvgh8oKp+ClwCvBJYC2wDPr6QJ06yPsl0kumZmZmF7CpJWoC+gj7J3syG/BVVdQ1AVT1cVU9X1TPAJ/nF8MxWYPWc3Vd1bb+kqjZU1VRVTU1MTAxyDpKkXehn1k2AS4F7qurCOe0r5mz2TuDObnkjcGKSfZIcCKwBbhleyZKkhehn1s0RwMnAHUk2dW3nASclWQsUcD/wHoCquivJVcDdzM7YOcMZN5I0Pj2Dvqq+DWSeVdftYp/zgfMHqEuSNCReGStJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxvVz90pJAmDynK88u3z/BceNsRIthD16SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXM+gT7I6yY1J7k5yV5Kzuvb9klyf5Afd95d27UnyiSSbk9ye5NBRn4Qkaef66dE/BZxdVQcBhwNnJDkIOAe4oarWADd0jwGOAdZ0X+uBS4ZetSSpbz2Dvqq2VdVt3fLjwD3ASmAdcHm32eXA8d3yOuCzNesmYN8kK4ZeuSSpLwsao08yCRwC3AwcUFXbulUPAQd0yyuBB+fstqVre+6x1ieZTjI9MzOzwLIlSf3q++6VSV4MfBH4QFX9NMmz66qqktRCnriqNgAbAKampha0r5Y/74Io7T599eiT7M1syF9RVdd0zQ/vGJLpvm/v2rcCq+fsvqprkySNQT+zbgJcCtxTVRfOWbUROLVbPhW4dk77Kd3sm8OBx+YM8UiSdrN+hm6OAE4G7kiyqWs7D7gAuCrJ6cADwAnduuuAY4HNwBPAaUOtWJK0ID2Dvqq+DWQnq4+aZ/sCzhiwLknSkHhlrCQ1zr8ZK42Qs4u0FNijl6TGGfSS1DiDXpIa5xi9NARzx+KlpcYevSQ1zqCXpMY5dKM9itMdtSeyRy9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcd4CQVpmvFOmFsoevSQ1rmfQJ7ksyfYkd85p+2iSrUk2dV/Hzll3bpLNSe5N8rZRFS5J6k8/PfrPAEfP035RVa3tvq4DSHIQcCLw2m6ff06y17CKlSQtXM+gr6pvAT/p83jrgCur6smq+iGwGThsgPokSQMa5MPYM5OcAkwDZ1fVI8BK4KY522zp2iQtU374u/wt9sPYS4BXAmuBbcDHF3qAJOuTTCeZnpmZWWQZkqReFhX0VfVwVT1dVc8An+QXwzNbgdVzNl3Vtc13jA1VNVVVUxMTE4spQ5LUh0UN3SRZUVXbuofvBHbMyNkIfC7JhcDLgTXALQNXqab55/2k0eoZ9Ek+DxwJ7J9kC/AR4Mgka4EC7gfeA1BVdyW5CrgbeAo4o6qeHk3pGjYDV2pTz6CvqpPmab50F9ufD5w/SFGSpOHxylhJapxBL0mNM+glqXHevVLz8oNZqR326CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjnF4pNcIpsdoZe/SS1DiDXpIa59CNmuSfv5N+wR69JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXE9gz7JZUm2J7lzTtt+Sa5P8oPu+0u79iT5RJLNSW5Pcugoi5ck9dZPj/4zwNHPaTsHuKGq1gA3dI8BjgHWdF/rgUuGU6YkabF63gKhqr6VZPI5zeuAI7vly4H/BD7UtX+2qgq4Kcm+SVZU1bZhFSxpafBumcvHYsfoD5gT3g8BB3TLK4EH52y3pWuTJI3JwB/Gdr33Wuh+SdYnmU4yPTMzM2gZkqSdWGzQP5xkBUD3fXvXvhVYPWe7VV3br6iqDVU1VVVTExMTiyxDktTLYoN+I3Bqt3wqcO2c9lO62TeHA485Pi9J49Xzw9gkn2f2g9f9k2wBPgJcAFyV5HTgAeCEbvPrgGOBzcATwGkjqFmStAD9zLo5aSerjppn2wLOGLQoSXJWz/B4ZawkNc6gl6TGGfSS1DiDXpIaZ9BLUuN6zrqRhmXuLApJu489eklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxXxkpaMrx6ejTs0UtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGDTS9Msn9wOPA08BTVTWVZD/gC8AkcD9wQlU9MliZkqTFGkaP/k1VtbaqprrH5wA3VNUa4IbusSRpTEYxdLMOuLxbvhw4fgTPIUnq06BXxhbw9SQF/EtVbQAOqKpt3fqHgAPm2zHJemA9wCte8YoBy2jf3CsG77/guDFWImm5GTTo/6CqtiZ5GXB9ku/PXVlV1b0J/IruTWEDwNTU1LzbSJIGN9DQTVVt7b5vB74EHAY8nGQFQPd9+6BFSpIWb9FBn+RFSV6yYxl4K3AnsBE4tdvsVODaQYuUJC3eIEM3BwBfSrLjOJ+rqq8l+Q5wVZLTgQeAEwYvU5K0WIsO+qq6Dzh4nvYfA0cNUpQkaXi8H72kPdKeNJPNoN/D+YcepPZD33vdSFLj7NFrSdmdPavn/jbTYk9OAoNefWj911qpdQa9JO0G4+wwGfSStJvt7tA36KXdxCEwjYtBv4Q59VHSMBj0kn6JHYz2OI9ekhpnj34AjrlKu4evtcHYo5ekxtmjl5YBx801CINeWiTDV8uFQS81yDFtzWXQL0O+iOdnD1uanx/GSlLj7NEvMfZKJQ2bQb9ABrH0qwYZTtydr6k99fXbVND7hySWnj31hSUtJSML+iRHAxcDewGfqqoLRvVckjQsw5zssFQ6OiMJ+iR7Af8EvAXYAnwnycaqunvYz7VUfpDLaSbMUvmZ9bK7f6bL6d9QWohR9egPAzZX1X0ASa4E1gFDD/pd8YUrSaML+pXAg3MebwHeMKLnGrlR94CXSw8bfPMclj3x5zis/+d74s9uUKmq4R80eRdwdFX9Rff4ZOANVXXmnG3WA+u7h68G7n3OYfYHfjT04sbP81pePK/lpdXzgvnP7beraqLXjqPq0W8FVs95vKpre1ZVbQA27OwASaaramo05Y2P57W8eF7LS6vnBYOd26iujP0OsCbJgUmeD5wIbBzRc0mSdmEkPfqqeirJmcB/MDu98rKqumsUzyVJ2rWRzaOvquuA6wY4xE6HdZY5z2t58byWl1bPCwY4t5F8GCtJWjq8e6UkNW7JB32S9yf5fpK7kvzduOsZpiRnJ6kk+4+7lmFI8vfdv9XtSb6UZN9x1zSIJEcnuTfJ5iTnjLueYUiyOsmNSe7uXlNnjbumYUqyV5LvJvnyuGsZliT7Jrm6e23dk+SNCz3Gkg76JG9i9orag6vqtcDHxlzS0CRZDbwV+N9x1zJE1wOvq6rfB/4bOHfM9SzanNt4HAMcBJyU5KDxVjUUTwFnV9VBwOHAGY2c1w5nAfeMu4ghuxj4WlW9BjiYRZzfkg564H3ABVX1JEBVbR9zPcN0EfBBoJkPSarq61X1VPfwJmavn1iunr2NR1X9HNhxG49lraq2VdVt3fLjzIbGyvFWNRxJVgHHAZ8ady3DkuQ3gD8CLgWoqp9X1aMLPc5SD/pXAX+Y5OYk/5Xk9eMuaBiSrAO2VtX3xl3LCP058NVxFzGA+W7j0UQg7pBkEjgEuHm8lQzNPzDbeXpm3IUM0YHADPDpbkjqU0letNCDjP1+9Em+AfzWPKs+zGx9+zH7K+brgauS/E4tg6lCPc7rPGaHbZadXZ1XVV3bbfNhZocIrtidtal/SV4MfBH4QFX9dNz1DCrJ24HtVXVrkiPHXc8QPQ84FHh/Vd2c5GLgHOCvFnqQsaqqP9nZuiTvA67pgv2WJM8we7+Hmd1V32Lt7LyS/B6z79LfSwKzwxu3JTmsqh7ajSUuyq7+vQCSvBt4O3DUcnhD3oWet/FYrpLszWzIX1FV14y7niE5AnhHkmOBFwC/nuTfqurPxlzXoLYAW6pqx29dVzMb9Auy1Idu/h14E0CSVwHPZ5nfsKiq7qiql1XVZFVNMvsPeehyCPleuj8280HgHVX1xLjrGVCTt/HIbO/iUuCeqrpw3PUMS1WdW1WrutfUicA3Gwh5ulx4MMmru6ajWMTt3sfeo+/hMuCyJHcCPwdOXea9xNb9I7APcH3328pNVfXe8Za0OA3fxuMI4GTgjiSburbzuivZtTS9H7ii63DcB5y20AN4ZawkNW6pD91IkgZk0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Lj/B1umGE0rI80LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NBIN = 100\n",
    "plt.hist(B1,NBIN)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the mean of the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.486340126658595\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "count = 0\n",
    "for b in B:\n",
    "    tmp = np.load('/Users/johnwelliaveetil/mastDownload/np_images'+'/' + b)\n",
    "    x = x + tmp.flatten().mean()\n",
    "print(x/len(B))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x/len(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the variance/standard deviation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.32743042934663\n"
     ]
    }
   ],
   "source": [
    "y = 0\n",
    "for b in B:\n",
    "    tmp = np.load('/Users/johnwelliaveetil/mastDownload/np_images'+'/' + b)\n",
    "    tmp = tmp.flatten()\n",
    "    tmp = (tmp - mean)**2\n",
    "    y = y + tmp.mean()\n",
    "var = y/len(B)\n",
    "std = np.sqrt(var)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataloader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the relevant pytorch tools..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f7fce10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the dataset folder class and add a numpy loader to it to create a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of channels \n",
    "nc = 1\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    " \n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "ngpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sample.reshape(1,64,64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_loader(path):\n",
    "    tmp = np.load(path)\n",
    "    tmp = (tmp - mean)/std\n",
    "    tmp = tmp.reshape(1,64,64)\n",
    "    sample = torch.from_numpy(tmp)\n",
    "    return sample\n",
    "    \n",
    "dataset = dset.DatasetFolder(\n",
    "    root= path0,\n",
    "    loader=npy_loader,\n",
    "    extensions=['.npy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/5][0/12216]\tLoss_D: 0.0029\tLoss_G: 10.5879\tD(x): 0.9984\tD(G(z)): 0.0013 / 0.0008\n",
      "[0/5][50/12216]\tLoss_D: 0.0023\tLoss_G: 9.0191\tD(x): 0.9982\tD(G(z)): 0.0005 / 0.0003\n",
      "[0/5][100/12216]\tLoss_D: 0.0029\tLoss_G: 7.7173\tD(x): 0.9990\tD(G(z)): 0.0019 / 0.0007\n",
      "[0/5][150/12216]\tLoss_D: 0.0018\tLoss_G: 8.7359\tD(x): 0.9990\tD(G(z)): 0.0007 / 0.0003\n",
      "[0/5][200/12216]\tLoss_D: 0.0019\tLoss_G: 8.1419\tD(x): 0.9990\tD(G(z)): 0.0009 / 0.0004\n",
      "[0/5][250/12216]\tLoss_D: 0.0013\tLoss_G: 8.3613\tD(x): 0.9993\tD(G(z)): 0.0007 / 0.0003\n",
      "[0/5][300/12216]\tLoss_D: 0.0011\tLoss_G: 8.3781\tD(x): 0.9994\tD(G(z)): 0.0005 / 0.0003\n",
      "[0/5][350/12216]\tLoss_D: 0.0020\tLoss_G: 8.3069\tD(x): 0.9990\tD(G(z)): 0.0010 / 0.0003\n",
      "[0/5][400/12216]\tLoss_D: 0.0013\tLoss_G: 9.1861\tD(x): 0.9988\tD(G(z)): 0.0001 / 0.0001\n",
      "[0/5][450/12216]\tLoss_D: 0.0070\tLoss_G: 10.9617\tD(x): 0.9959\tD(G(z)): 0.0024 / 0.0004\n",
      "[0/5][500/12216]\tLoss_D: 0.0474\tLoss_G: 9.2248\tD(x): 0.9717\tD(G(z)): 0.0145 / 0.0064\n",
      "[1/5][0/12216]\tLoss_D: 0.0756\tLoss_G: 10.6803\tD(x): 0.9877\tD(G(z)): 0.0590 / 0.0002\n",
      "[1/5][50/12216]\tLoss_D: 0.0073\tLoss_G: 6.6976\tD(x): 0.9965\tD(G(z)): 0.0038 / 0.0023\n",
      "[1/5][100/12216]\tLoss_D: 0.0182\tLoss_G: 7.0962\tD(x): 0.9911\tD(G(z)): 0.0091 / 0.0015\n",
      "[1/5][150/12216]\tLoss_D: 0.0035\tLoss_G: 7.9572\tD(x): 0.9980\tD(G(z)): 0.0015 / 0.0010\n",
      "[1/5][200/12216]\tLoss_D: 0.0208\tLoss_G: 9.8770\tD(x): 0.9974\tD(G(z)): 0.0176 / 0.0003\n",
      "[1/5][250/12216]\tLoss_D: 0.0054\tLoss_G: 10.3302\tD(x): 0.9948\tD(G(z)): 0.0001 / 0.0001\n",
      "[1/5][300/12216]\tLoss_D: 0.0022\tLoss_G: 8.0668\tD(x): 0.9985\tD(G(z)): 0.0007 / 0.0006\n",
      "[1/5][350/12216]\tLoss_D: 0.0007\tLoss_G: 10.2832\tD(x): 0.9994\tD(G(z)): 0.0001 / 0.0001\n",
      "[1/5][400/12216]\tLoss_D: 0.0022\tLoss_G: 8.0145\tD(x): 0.9995\tD(G(z)): 0.0017 / 0.0009\n",
      "[1/5][450/12216]\tLoss_D: 0.0013\tLoss_G: 8.3281\tD(x): 0.9991\tD(G(z)): 0.0004 / 0.0003\n",
      "[1/5][500/12216]\tLoss_D: 0.0026\tLoss_G: 7.2242\tD(x): 0.9994\tD(G(z)): 0.0020 / 0.0011\n",
      "[2/5][0/12216]\tLoss_D: 0.0028\tLoss_G: 7.1381\tD(x): 0.9994\tD(G(z)): 0.0021 / 0.0012\n",
      "[2/5][50/12216]\tLoss_D: 0.0003\tLoss_G: 10.9316\tD(x): 0.9998\tD(G(z)): 0.0000 / 0.0000\n",
      "[2/5][100/12216]\tLoss_D: 0.0005\tLoss_G: 10.2306\tD(x): 0.9997\tD(G(z)): 0.0001 / 0.0001\n",
      "[2/5][150/12216]\tLoss_D: 0.0016\tLoss_G: 7.9554\tD(x): 0.9992\tD(G(z)): 0.0008 / 0.0006\n",
      "[2/5][200/12216]\tLoss_D: 0.0025\tLoss_G: 7.3434\tD(x): 0.9992\tD(G(z)): 0.0017 / 0.0011\n",
      "[2/5][250/12216]\tLoss_D: 0.0004\tLoss_G: 9.2433\tD(x): 0.9998\tD(G(z)): 0.0002 / 0.0002\n",
      "[2/5][300/12216]\tLoss_D: 0.0018\tLoss_G: 8.0908\tD(x): 0.9989\tD(G(z)): 0.0007 / 0.0005\n",
      "[2/5][350/12216]\tLoss_D: 0.0014\tLoss_G: 7.9103\tD(x): 0.9999\tD(G(z)): 0.0013 / 0.0008\n",
      "[2/5][400/12216]\tLoss_D: 0.0017\tLoss_G: 8.4076\tD(x): 0.9988\tD(G(z)): 0.0004 / 0.0004\n",
      "[2/5][450/12216]\tLoss_D: 0.0084\tLoss_G: 10.2093\tD(x): 0.9996\tD(G(z)): 0.0078 / 0.0001\n",
      "[2/5][500/12216]\tLoss_D: 0.0019\tLoss_G: 9.6839\tD(x): 0.9986\tD(G(z)): 0.0005 / 0.0003\n",
      "[3/5][0/12216]\tLoss_D: 0.0017\tLoss_G: 8.5558\tD(x): 0.9993\tD(G(z)): 0.0009 / 0.0006\n",
      "[3/5][50/12216]\tLoss_D: 0.0024\tLoss_G: 7.6076\tD(x): 0.9995\tD(G(z)): 0.0018 / 0.0008\n",
      "[3/5][100/12216]\tLoss_D: 0.0005\tLoss_G: 9.7771\tD(x): 0.9996\tD(G(z)): 0.0001 / 0.0001\n",
      "[3/5][150/12216]\tLoss_D: 0.0005\tLoss_G: 12.4631\tD(x): 0.9997\tD(G(z)): 0.0001 / 0.0000\n",
      "[3/5][200/12216]\tLoss_D: 0.0001\tLoss_G: 43.3702\tD(x): 0.9999\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][250/12216]\tLoss_D: 0.0001\tLoss_G: 41.8070\tD(x): 0.9999\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][300/12216]\tLoss_D: 0.0000\tLoss_G: 41.7335\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][350/12216]\tLoss_D: 0.0000\tLoss_G: 41.7408\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][400/12216]\tLoss_D: 0.0000\tLoss_G: 41.7436\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][450/12216]\tLoss_D: 0.0000\tLoss_G: 41.6937\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[3/5][500/12216]\tLoss_D: 0.0000\tLoss_G: 41.4057\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][0/12216]\tLoss_D: 0.0000\tLoss_G: 41.3510\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][50/12216]\tLoss_D: 0.0000\tLoss_G: 41.1994\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][100/12216]\tLoss_D: 0.0000\tLoss_G: 41.2608\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][150/12216]\tLoss_D: 0.0000\tLoss_G: 41.4582\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][200/12216]\tLoss_D: 0.0000\tLoss_G: 41.1634\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][250/12216]\tLoss_D: 0.0000\tLoss_G: 41.3586\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][300/12216]\tLoss_D: 0.0005\tLoss_G: 41.0534\tD(x): 0.9995\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][350/12216]\tLoss_D: 0.0000\tLoss_G: 40.9103\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][400/12216]\tLoss_D: 0.0000\tLoss_G: 40.4255\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][450/12216]\tLoss_D: 0.0001\tLoss_G: 40.9091\tD(x): 0.9999\tD(G(z)): 0.0000 / 0.0000\n",
      "[4/5][500/12216]\tLoss_D: 0.0000\tLoss_G: 40.8507\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        if i <= 500:\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            ## Train with all-real batch\n",
    "            netD.zero_grad()\n",
    "            # Format batch\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size,), real_label, device=device)\n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            ## Train with all-fake batch\n",
    "            # Generate batch of latent vectors\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            # Generate fake image batch with G\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            # Classify all fake batch with D\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label)\n",
    "            # Calculate the gradients for this batch\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fake).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            errG = criterion(output, label)\n",
    "            # Calculate gradients for G\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "    \n",
    "            # Output training stats\n",
    "            if i % 50 == 0:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "    \n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "    \n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "            if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "                with torch.no_grad():\n",
    "                    fake = netG(fixed_noise).detach().cpu()\n",
    "                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "    \n",
    "            iters += 1\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
